# AI Models in `safarX/ai/`

This directory contains two core AI anomaly detection models and supporting scripts used in the safarX project. Below, you'll find an overview of the models, their purposes, the technology stack, the methodology for synthetic data generation, and a breakdown of the functionality of each file. Included Below is also the methodology used to calculate the SafetyScore

---

## 1. Overview of the AI Models

### a. Prolonged Inactivity Detection Model
- **Purpose**: Detects abnormal periods of inactivity in user activity data to identify potential safety issues or disengagement.
- **Files**: 
  - `ProlongedInactivityDATA.py`
  - `ProlongedInactivityMODEL.py`
  - `ProlongedInactivityTEST.py`

### b. Drop-off Event Detection Model
- **Purpose**: Identifies anomalous drop-off events (e.g., unexpected or unsafe drop-off locations) in terms of GPS/Location.
- **Files**:
  - `dropoff-data.py`
  - `dropoff-model.py`
  - `dropoff-test.py`

---

## 2. Libraries, Packages, and Frameworks Used

| Library/Package         | Purpose                                                                |
|------------------------ |-----------------------------------------------------------------------|
| `scikit-learn`          | Core machine learning algorithms (IsolationForest, preprocessing)      |
| `numpy`                 | Numerical operations and data manipulation                             |
| `pandas`                | Data loading, handling, and manipulation                               |
| `joblib`                | Model and transformer serialization                                    |
| `flask` (in safetyscore)| API endpoint creation for safety score inference                      |
| `os`, `sys`             | File handling and system interaction                                   |
| `numpy + random`                | Synthetic data generation                                             |
| `matplotlib`                | Synthetic data analysis                                             |

---

## 3. What is IsolationForest and Why Was It Chosen?

**IsolationForest** is an unsupervised anomaly detection algorithm that isolates anomalies instead of profiling normal data points. It works by recursively partitioning data using random splits; anomalies are more likely to be isolated quickly due to being few and different. This makes it highly efficient and effective for high-dimensional data and large datasets.

**Why chosen**:
- Does not require labeled data (ideal due to lack of real-world labels).
- Robust to outliers and effective for anomaly detection tasks.
- Fast and scalable for production use.

---

## 4. Synthetic Data Generation Methodology

**Why Synthetic Data?**  
Due to the lack of high-quality, labeled real-world datasets for these anomaly detection tasks, synthetic data was generated to train and evaluate the models.

Synthetic data was generated with real world trends and patterns taken into consideration with regards to tourist behaviour

### a. For Prolonged Inactivity Detection
- Synthetic user activity logs were generated by simulating normal usage patterns (e.g., regular log-ins, activity bursts) as well as anomalous inactivity periods (e.g., long gaps, sudden stops).
- Features included: app usage, displacement, time, arearisk and other important parameters.
- Randomized parameters and injected anomalous patterns to ensure the model learns to distinguish between typical and atypical behaviors.

### b. For Drop-off Event Detection
- Location data and drop-off events were synthetically created to resemble normal drop-offs and injected with rare, abnormal events (e.g., drop-offs in unsafe or unexpected locations).
- Features included: GPS, Internet connectivity and contextual data.
- Anomalies were simulated by generating drop-offs outside typical operating zones or at odd times.

### Mathematical Methodology

All synthetic data in these models is created using well-known probability distributions that describe how real-world events typically occur. Here’s a more detailed breakdown:

- **Standard Probability Distributions:**  
  Each feature or event in the dataset is generated by sampling from mathematical distributions:
  - **Bernoulli Distribution:** Used for yes/no or true/false (binary) events. For example, whether a device has connectivity or not, with a set probability of success.
  - **Categorical Distribution:** Used when picking from several possible categories, like area risk being 'low', 'medium', or 'high', each with a specific probability.
  - **Poisson Distribution:** Used to model how many times an event happens in a fixed interval, like the number of sudden movements in a time period.
  - **Exponential Distribution:** Used for modeling waiting times between events, such as the time since the last successful ping.
  - **Normal (Gaussian) Distribution:** Used for values that cluster around a mean, like sensor readings or measurement noise.

- **Parameter Selection:**  
  The parameters (like mean, variance, probability weights) for these distributions are carefully chosen to mimic realistic frequencies and patterns seen in actual user or device data. For example, most samples will be generated to reflect normal activity, but parameters are tweaked to match expected real-world skews (e.g., most times are short, but a few are very long).

- **Injecting Anomalies:**  
  To ensure the dataset includes rare or abnormal events (which are critical for training anomaly detectors), the data generation process intentionally:
    - **Biases the tail probabilities:** Increases the chance of generating extreme values (e.g., very long inactivity periods, very bad network conditions).
    - **Explicitly samples rare cases:** Directly inserts outlier or unlikely events at a controlled rate, even if they would be extremely rare under normal distribution parameters.

This approach results in synthetic data that not only looks statistically similar to real-world data but also includes enough rare, abnormal examples to robustly train and evaluate anomaly detection models.
---

## 5. File Functionality Breakdown

| File                              | Functionality                                                                                  |
|------------------------------------|-----------------------------------------------------------------------------------------------|
| `ProlongedInactivityDATA.py`       | Generates, preprocesses, and organizes user inactivity data, both normal and anomalous         |
| `ProlongedInactivityMODEL.py`      | Trains IsolationForest model for inactivity anomaly detection, handles model persistence       |
| `ProlongedInactivityTEST.py`       | Tests the inactivity model using synthetic/real data, evaluates its performance                |
| `dropoff-data.py`                  | Prepares and augments drop-off event data, including synthetic anomaly injection               |
| `dropoff-model.py`                 | Trains IsolationForest for drop-off anomaly detection, saves model artifacts                   |
| `dropoff-test.py`                  | Evaluates drop-off model on test data, measures detection accuracy and false positives         |
| `main.py`                          | Entrypoint for running models, managing workflow between data, model, and inference            |
| `model_handler.py`                 | Loads models, scales data, and provides inference utilities                                    |
| `safetyscore/` (directory)         | Contains API logic for exposing safety score via REST endpoints                                |
| `.joblib` files                    | Serialized model and scaler objects for both activity and drop-off models                      |
| `.csv` files                       | Example datasets and test data used for model development and validation                       |

---

# Safety Score Module

Calculates a location's safety score (0–100, higher is safer) using:

## APIs Used

- **OpenStreetMap Overpass API:** Finds nearest amenities (roads, hospitals, police, etc.) for accessibility scoring.
- **met.no Weather API:** Gets weather hazards for environmental risk.

## How Scoring Works

The score combines four factors:

1. **Remoteness (Cell Tower Density):**  
   - Uses local CSV of cell tower locations (“404.csv”)
   - Counts towers within 0.5, 1, 5, 15 km, normalizes for density

2. **Accessibility:**  
   - Uses Overpass API to find distance to nearest road, hospital, police, fuel, ATM, pharmacy, hotel
   - Closer means safer; distances normalized and weighted

3. **Environmental Hazard:**  
   - Uses met.no weather API
   - Bad weather (storms, heavy rain, etc.) increases risk

4. **Geofence:**  
   - Boolean input: 1.0 if geofenced area (safer), 0.0 otherwise

**Weights:**  
Remoteness 0.2, Accessibility 0.2, Environment 0.2, Geofence 0.4

**Score Formula:**  
```
weighted_score = w1*remoteness + w2*accessibility + w3*environment + w4*geofence  
safety_score = (1 - weighted_score) * 100
```
**Risk Levels:**  
- 80–100: Low risk  
- 40–79: Medium  
- <40: High
---

