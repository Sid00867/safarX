# AI Models in `safarX/ai/`

This directory contains two core AI anomaly detection models and supporting scripts used in the safarX project. Below, you'll find an overview of the models, their purposes, the technology stack, the methodology for synthetic data generation, and a breakdown of the functionality of each file. Included Below is also the methodology used to calculate the SafetyScore

---

## 1. Overview of the AI Models

### a. Prolonged Inactivity Detection Model
- **Purpose**: Detects abnormal periods of inactivity in user activity data to identify potential safety issues or disengagement.
- **Files**: 
  - `ProlongedInactivityDATA.py`
  - `ProlongedInactivityMODEL.py`
  - `ProlongedInactivityTEST.py`

### b. Drop-off Event Detection Model
- **Purpose**: Identifies anomalous drop-off events (e.g., unexpected or unsafe drop-off locations) in terms of GPS/Location.
- **Files**:
  - `dropoff-data.py`
  - `dropoff-model.py`
  - `dropoff-test.py`

---

## 2. Libraries, Packages, and Frameworks Used

| Library/Package         | Purpose                                                                |
|------------------------ |-----------------------------------------------------------------------|
| `scikit-learn`          | Core machine learning algorithms (IsolationForest, preprocessing)      |
| `numpy`                 | Numerical operations and data manipulation                             |
| `pandas`                | Data loading, handling, and manipulation                               |
| `joblib`                | Model and transformer serialization                                    |
| `flask` (in safetyscore)| API endpoint creation for safety score inference                      |
| `os`, `sys`             | File handling and system interaction                                   |
| `random`                | Synthetic data generation                                             |

---

## 3. What is IsolationForest and Why Was It Chosen?

**IsolationForest** is an unsupervised anomaly detection algorithm that isolates anomalies instead of profiling normal data points. It works by recursively partitioning data using random splits; anomalies are more likely to be isolated quickly due to being few and different. This makes it highly efficient and effective for high-dimensional data and large datasets.

**Why chosen**:
- Does not require labeled data (ideal due to lack of real-world labels).
- Robust to outliers and effective for anomaly detection tasks.
- Fast and scalable for production use.

---

## 4. Synthetic Data Generation Methodology

**Why Synthetic Data?**  
Due to the lack of high-quality, labeled real-world datasets for these anomaly detection tasks, synthetic data was generated to train and evaluate the models.

### a. For Prolonged Inactivity Detection
- Synthetic user activity logs were generated by simulating normal usage patterns (e.g., regular check-ins, activity bursts) as well as anomalous inactivity periods (e.g., long gaps, sudden stops).
- Features included: timestamp sequences, activity type, duration, and idle intervals.
- Randomized parameters and injected anomalous patterns to ensure the model learns to distinguish between typical and atypical behaviors.

### b. For Drop-off Event Detection
- Location data and drop-off events were synthetically created to resemble normal drop-offs and injected with rare, abnormal events (e.g., drop-offs in unsafe or unexpected locations).
- Features included: latitude, longitude, time of day, and contextual tags (e.g., 'urban', 'isolated').
- Anomalies were simulated by generating drop-offs outside typical operating zones or at odd times.

**Emphasis**:  
The synthetic data generation scripts (`generate_anamolous_data.py`, scripts within the model/data files) are customizable to tune the frequency, magnitude, and distribution of anomalies, providing a controlled environment for model training and validation.

---

## 5. File Functionality Breakdown

| File                              | Functionality                                                                                  |
|------------------------------------|-----------------------------------------------------------------------------------------------|
| `ProlongedInactivityDATA.py`       | Generates, preprocesses, and organizes user inactivity data, both normal and anomalous         |
| `ProlongedInactivityMODEL.py`      | Trains IsolationForest model for inactivity anomaly detection, handles model persistence       |
| `ProlongedInactivityTEST.py`       | Tests the inactivity model using synthetic/real data, evaluates its performance                |
| `dropoff-data.py`                  | Prepares and augments drop-off event data, including synthetic anomaly injection               |
| `dropoff-model.py`                 | Trains IsolationForest for drop-off anomaly detection, saves model artifacts                   |
| `dropoff-test.py`                  | Evaluates drop-off model on test data, measures detection accuracy and false positives         |
| `generate_anamolous_data.py`       | Utility for creating synthetic datasets with configurable anomaly parameters                   |
| `main.py`                          | Entrypoint for running models, managing workflow between data, model, and inference            |
| `model_handler.py`                 | Loads models, scales data, and provides inference utilities                                    |
| `safetyscore/` (directory)         | Contains API logic for exposing safety score via REST endpoints                                |
| `.joblib` files                    | Serialized model and scaler objects for both activity and drop-off models                      |
| `.csv` files                       | Example datasets and test data used for model development and validation                       |

---

## 6. Safety Score API: Outline & Methodology

- **API Location**: `ai/safetyscore/`
- **Purpose**: Provides REST endpoints to score user activity or drop-off events for safety in real time.
- **Methodology**:
    - Receives user or event data via API POST requests.
    - Preprocesses the input using the same transformers (scalers, encoders) as used in training.
    - Feeds the data into the corresponding IsolationForest model.
    - Returns a safety score based on the anomaly score or binary classification (normal/anomaly).
    - Implements rate limiting and input validation for robust, secure operation.

---

## 7. Datasets Used & Aggregation

- **Synthetic datasets** were created to emulate real-world user activity and drop-off event patterns.
- Data aggregation involved generating multiple users/events, each with a blend of normal and anomalous sequences.
- CSV files (`user_activity_data.csv`, `dropoff_data.csv`, `tourist_safety_dataset_test.csv`) aggregate these synthetic records.
- This approach ensures a diverse, balanced dataset that covers a wide range of scenarios, supporting robust model evaluation.

---

## Summary

The AI models in this directory are tailored for unsupervised anomaly detection in user activity and drop-off events, using synthetic data to overcome the lack of high-quality labeled data. The core methodology emphasizes flexibility and realism in data generation, leveraging IsolationForest for efficient, robust anomaly detection, and exposing results through a scalable API.

For further details, refer to the code and comments in each script.
